{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "The idea behind PCA is to transform our dataset into something more useful for building models. What we want to do is to build new dimensions (predictors) out of the dimensions we are given in such a way that:\n",
    "\n",
    "(1) each dimension we draw captures as much of the remaining variance among our predictors as possible; and <br/>\n",
    "(2) each dimension we draw is orthogonal to the ones we've already drawn.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Think back to multiple linear regression for a moment.\n",
    "\n",
    "The fundamental idea is that I can get a better prediction for my dependent variable by considering a *linear combination of my predictors* than I can get by considering any one predictor by itself.\n",
    "\n",
    "$\\rightarrow$ **PCA insight**: If the combinations of predictors work better than the predictors themselves, then let's just treat the combinations as our primary dimensions!\n",
    "\n",
    "But one problem with having lots of predictors is that it raises the chance that some will be nearly *collinear*.\n",
    "\n",
    "$\\rightarrow$ **PCA insight**: Since we're reconstructing our dimensions anyway, let's make sure that the dimensions we construct are mutually orthogonal! <br/>\n",
    "$\\rightarrow$ **PCA insight**: Moreover, since we'll be capturing much of the variance among our predictors in the first few dimensions we construct, we'll be able in effect to *reduce  the dimensionality* of our problem. Thus PCA is a fundamental tool in *dimensionality reduction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('../../randomforest/Random-Forest_tree_ensembles/cars.csv')\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[' cubicinches'].replace(' ', np.nan, inplace=True)\n",
    "cars[' cubicinches'] = cars[' cubicinches'].map(float)\n",
    "cars[' cubicinches'].fillna(cars[' cubicinches'].mean(skipna=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[' weightlbs'].replace(' ', np.nan, inplace=True)\n",
    "cars[' weightlbs'] = cars[' weightlbs'].map(float)\n",
    "cars[' weightlbs'].fillna(cars[' weightlbs'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[' cylinders'] = cars[' cylinders'].map(float)\n",
    "cars[' hp'] = cars[' hp'].map(float)\n",
    "cars[' time-to-60'] = cars[' time-to-60'].map(float)\n",
    "cars[' year'] = cars[' year'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our predictors and target\n",
    "X = cars.drop([' brand','mpg'], axis=1)\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "ss = StandardScaler().fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale-transforming\n",
    "\n",
    "X_tr_sc = ss.transform(X_train)\n",
    "X_ts_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's construct a linear regression\n",
    "lr = LinearRegression().fit(X_tr_sc,y_train)\n",
    "# Score on train\n",
    "lr.score(X_tr_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on test\n",
    "lr.score(X_ts_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the best-fit hyperplane\n",
    "lr.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-1.404\\times cyl + 0.669\\times in^3 - 0.482\\times hp - 4.651\\times lbs. -  0.177\\times time_{60} + 2.425\\times yr$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "The key idea is to diagonalize (i.e. find the eigendecomposition of) the covariance matrix. The decomposition will produce a set of orthogonal vectors that explain as much of the remaining variance as possible.\n",
    "\n",
    "Let's say a word about eigenvalues and eigenvectors. It turns out that eigenvalues and -vectors have a dizzying number of applications. But the basic idea is that, if we can split a bunch of vectors (i.e. a matrix) into a set of mutually orthogonal vectors, then we can isolate the force of the bunch into discrete bits, each of which by itself acts like a simple linear transformation.\n",
    "\n",
    "That's why the definition of an eigenvector is as it is: $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some scalar $\\lambda$. That is, the vector is oriented in just such a direction that multiplying the matrix by it serves only to lengthen or shorten it.\n",
    "\n",
    "Let's do a simple example.\n",
    "\n",
    "Suppose we have the matrix\n",
    "$A =\n",
    "\\begin{bmatrix}\n",
    "1 & 0.6 \\\\\n",
    "0.6 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Let's calculate the eigendecomposition of this matrix.\n",
    "\n",
    "\n",
    "## PCA by Hand\n",
    "[Here's](https://www.youtube.com/watch?v=_UVHneBUBW0) a video introduction to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = [[1,0.6],[0.6,1]]\n",
    "\n",
    "values, vectors = np.linalg.eig(A) #eigen vector is the output vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.dot(np.diag(values)).dot(vectors.T )   #should get back A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: What follows is indebted to http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by producing the covariance matrix for the columns of X.\n",
    "cov_mat = np.cov(X_tr_sc.T) #shows the relationship among the cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.linalg.eig(X) returns a double of NumPy arrays, the first containing\n",
    "# the eigenvalues of X and the second containing the eigenvectors of X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assign the results of eig(cov_mat) to a double of variables.\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now pair up each eigenvalue with its corresponding eigenvector.\n",
    "\n",
    "eigpairs = [(eigvals[i], eigvecs[:,i]) for i in range(len(eigvals))]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first element of eigpairs.\n",
    "\n",
    "eigpairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second element of each element in eigpairs is\n",
    "# an eigenvector of the covariance matrix.\n",
    "\n",
    "eigpairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to isolate the eigenvectors and create a matrix\n",
    "# with a column for each. We'll use just three of these,\n",
    "# corresponding to taking the first three principal components. \n",
    "# they are ordered so the first three are the most significant\n",
    "\n",
    "pcabh = np.hstack((eigpairs[0][1].reshape(6,1),\n",
    "                  eigpairs[1][1].reshape(6,1),\n",
    "                  eigpairs[2][1].reshape(6,1)))\n",
    "pcabh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we simply want the dot-product of\n",
    "# X (scaled) with this matrix of the eigenvectors\n",
    "# of the covariance matrix of X.\n",
    "\n",
    "X_tr_sc.dot(pcabh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naturally, sklearn has a shortcut for this!\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_train_new = pca.fit_transform(X_tr_sc)\n",
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the explained variance\n",
    "pca.explained_variance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio is often more informative\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can also check out the Principal Components themselves\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the columns of X\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our PCA are as follows:\n",
    "\n",
    "**PC1** = 0.454 * cylinders + 0.470 * cubicinches + 0.462 * hp + 0.440 * weightlbs - 0.357 * time-to-60 - 0.196 * year\n",
    "\n",
    "**PC2** = -0.143 * cylinders - 0.110 * cubicinches - 0.023 * hp - 0.217 * weightlbs - 0.102 * time-to-60 - 0.954 * year\n",
    "\n",
    "**PC3** = 0.204 * cylinders + 0.153 * cubicinches - 0.129 * hp + 0.361 * weightlbs + 0.860 * time-to-60 - 0.220 * year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that these loadings are encoded in the eigenvectors of $X^TX$. Notice that:\n",
    "\n",
    "- the absolute values of the components of PC1 are the first components of the eigenvectors below, <br/>\n",
    "- the absolute values of the components of PC2 are the second components of the eigenvectors below, <br/>\n",
    "- etc. <br/>\n",
    "\n",
    "We'll have more to say about this when we examine the singular value decomposition of matrices in Mod 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eig(X_tr_sc.T.dot(X_tr_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These principal components should be normalized.\n",
    "# If they are, then the sum of the squares of the\n",
    "# loadings should be 1. Let's check!\n",
    "\n",
    "mag0 = 0 \n",
    "for i in range(6):\n",
    "    mag0+=pca.components_[0][i]**2\n",
    "mag0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag1 = 0 \n",
    "for i in range(6):\n",
    "    mag1+=pca.components_[1][i]**2\n",
    "mag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag2 = 0 \n",
    "for i in range(6):\n",
    "    mag2+=pca.components_[2][i]**2\n",
    "mag2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These principal components should also be\n",
    "# mutually orthogonal. If they are, then the\n",
    "# dot product of any two of them should be 0.\n",
    "# Let's check!\n",
    "\n",
    "dot_prod01 = 0\n",
    "for i in range(6):\n",
    "    dot_prod01+=pca.components_[0][i] * pca.components_[1][i]\n",
    "dot_prod01    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod02 = 0\n",
    "for i in range(6):\n",
    "    dot_prod02+=pca.components_[0][i] * pca.components_[2][i]\n",
    "dot_prod02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod12 = 0\n",
    "for i in range(6):\n",
    "    dot_prod12+=pca.components_[1][i] * pca.components_[2][i]\n",
    "dot_prod12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train_new[:, 0], y_train, 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train_new[:, 1], y_train, 'g.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train_new[:, 2], y_train, 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Is the first principal component the same line we would get if we constructed an ordinary least-squares regression line?\n",
    "\n",
    "The answer is NO! Check out this post for an illuminating discussion: https://shankarmsy.github.io/posts/pca-vs-lr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with New Dimensions\n",
    "\n",
    "Now that we have optimized our features, we can build a new model with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_new, y_train)\n",
    "lr_pca.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = pca.transform(X_ts_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca.coef_    #-3.00 is the coefficient of the model but 0.454 was the coefficient of transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-3.00\\times PC1 - 1.15\\times PC2 -2.49\\times PC3$\n",
    "\n",
    "Of course, since the principal components are just linear combinations of our original predictors, we could re-express this hyperplane in terms of those original predictors!\n",
    "\n",
    "And if the PCA was worth anything, we should expect the new linear model to be *different from* the first!\n",
    "\n",
    "Recall that we had:\n",
    "\n",
    "**PC1** = 0.454 * cylinders + 0.470 * cubicinches + 0.462 * hp + 0.440 * weightlbs - 0.357 * time-to-60 - 0.196 * year\n",
    "\n",
    "**PC2** = -0.143 * cylinders - 0.110 * cubicinches - 0.023 * hp - 0.217 * weightlbs - 0.102 * time-to-60 - 0.954 * year\n",
    "\n",
    "**PC3** = 0.204 * cylinders + 0.153 * cubicinches - 0.129 * hp + 0.361 * weightlbs + 0.860 * time-to-60 - 0.220 * year\n",
    "\n",
    "Therefore, our new PCA-made hyperplane can be expressed as:\n",
    "\n",
    "$-3.00\\times(0.454\\times cyl + 0.470\\times in^3 + 0.462\\times hp + 0.440\\times lbs. - 0.357\\times time_{60} - 0.196\\times yr)$ <br/> $- 1.15\\times(-0.143\\times cyl - 0.110\\times in^3 - 0.023\\times hp - 0.217\\times lbs. - 0.102\\times time_{60} - 0.954\\times yr)$ <br/> $- 2.49\\times(0.204\\times cyl + 0.153\\times in^3 - 0.129\\times hp + 0.361\\times lbs. + 0.860\\times time_{60} - 0.220\\times yr)$ <br/><br/> $= -1.706\\times cyl - 1.664\\times in^3 -1.038\\times hp - 1.969\\times lbs. -3.095\\times time_{60} + 2.233\\times yr$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first linear regression model had:\n",
    "\n",
    "$-1.404\\times cyl + 0.671\\times in^3 - 0.478\\times hp - 4.655\\times lbs. -  0.177\\times time_{60} + 2.426\\times yr$,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
